{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea6983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8feb98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e903ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01',\n",
       " 'ca02',\n",
       " 'ca03',\n",
       " 'ca04',\n",
       " 'ca05',\n",
       " 'ca06',\n",
       " 'ca07',\n",
       " 'ca08',\n",
       " 'ca09',\n",
       " 'ca10',\n",
       " 'ca11',\n",
       " 'ca12',\n",
       " 'ca13',\n",
       " 'ca14',\n",
       " 'ca15',\n",
       " 'ca16',\n",
       " 'ca17',\n",
       " 'ca18',\n",
       " 'ca19',\n",
       " 'ca20',\n",
       " 'ca21',\n",
       " 'ca22',\n",
       " 'ca23',\n",
       " 'ca24',\n",
       " 'ca25',\n",
       " 'ca26',\n",
       " 'ca27',\n",
       " 'ca28',\n",
       " 'ca29',\n",
       " 'ca30',\n",
       " 'ca31',\n",
       " 'ca32',\n",
       " 'ca33',\n",
       " 'ca34',\n",
       " 'ca35',\n",
       " 'ca36',\n",
       " 'ca37',\n",
       " 'ca38',\n",
       " 'ca39',\n",
       " 'ca40',\n",
       " 'ca41',\n",
       " 'ca42',\n",
       " 'ca43',\n",
       " 'ca44',\n",
       " 'cb01',\n",
       " 'cb02',\n",
       " 'cb03',\n",
       " 'cb04',\n",
       " 'cb05',\n",
       " 'cb06',\n",
       " 'cb07',\n",
       " 'cb08',\n",
       " 'cb09',\n",
       " 'cb10',\n",
       " 'cb11',\n",
       " 'cb12',\n",
       " 'cb13',\n",
       " 'cb14',\n",
       " 'cb15',\n",
       " 'cb16',\n",
       " 'cb17',\n",
       " 'cb18',\n",
       " 'cb19',\n",
       " 'cb20',\n",
       " 'cb21',\n",
       " 'cb22',\n",
       " 'cb23',\n",
       " 'cb24',\n",
       " 'cb25',\n",
       " 'cb26',\n",
       " 'cb27',\n",
       " 'cc01',\n",
       " 'cc02',\n",
       " 'cc03',\n",
       " 'cc04',\n",
       " 'cc05',\n",
       " 'cc06',\n",
       " 'cc07',\n",
       " 'cc08',\n",
       " 'cc09',\n",
       " 'cc10',\n",
       " 'cc11',\n",
       " 'cc12',\n",
       " 'cc13',\n",
       " 'cc14',\n",
       " 'cc15',\n",
       " 'cc16',\n",
       " 'cc17',\n",
       " 'cd01',\n",
       " 'cd02',\n",
       " 'cd03',\n",
       " 'cd04',\n",
       " 'cd05',\n",
       " 'cd06',\n",
       " 'cd07',\n",
       " 'cd08',\n",
       " 'cd09',\n",
       " 'cd10',\n",
       " 'cd11',\n",
       " 'cd12',\n",
       " 'cd13',\n",
       " 'cd14',\n",
       " 'cd15',\n",
       " 'cd16',\n",
       " 'cd17',\n",
       " 'ce01',\n",
       " 'ce02',\n",
       " 'ce03',\n",
       " 'ce04',\n",
       " 'ce05',\n",
       " 'ce06',\n",
       " 'ce07',\n",
       " 'ce08',\n",
       " 'ce09',\n",
       " 'ce10',\n",
       " 'ce11',\n",
       " 'ce12',\n",
       " 'ce13',\n",
       " 'ce14',\n",
       " 'ce15',\n",
       " 'ce16',\n",
       " 'ce17',\n",
       " 'ce18',\n",
       " 'ce19',\n",
       " 'ce20',\n",
       " 'ce21',\n",
       " 'ce22',\n",
       " 'ce23',\n",
       " 'ce24',\n",
       " 'ce25',\n",
       " 'ce26',\n",
       " 'ce27',\n",
       " 'ce28',\n",
       " 'ce29',\n",
       " 'ce30',\n",
       " 'ce31',\n",
       " 'ce32',\n",
       " 'ce33',\n",
       " 'ce34',\n",
       " 'ce35',\n",
       " 'ce36',\n",
       " 'cf01',\n",
       " 'cf02',\n",
       " 'cf03',\n",
       " 'cf04',\n",
       " 'cf05',\n",
       " 'cf06',\n",
       " 'cf07',\n",
       " 'cf08',\n",
       " 'cf09',\n",
       " 'cf10',\n",
       " 'cf11',\n",
       " 'cf12',\n",
       " 'cf13',\n",
       " 'cf14',\n",
       " 'cf15',\n",
       " 'cf16',\n",
       " 'cf17',\n",
       " 'cf18',\n",
       " 'cf19',\n",
       " 'cf20',\n",
       " 'cf21',\n",
       " 'cf22',\n",
       " 'cf23',\n",
       " 'cf24',\n",
       " 'cf25',\n",
       " 'cf26',\n",
       " 'cf27',\n",
       " 'cf28',\n",
       " 'cf29',\n",
       " 'cf30',\n",
       " 'cf31',\n",
       " 'cf32',\n",
       " 'cf33',\n",
       " 'cf34',\n",
       " 'cf35',\n",
       " 'cf36',\n",
       " 'cf37',\n",
       " 'cf38',\n",
       " 'cf39',\n",
       " 'cf40',\n",
       " 'cf41',\n",
       " 'cf42',\n",
       " 'cf43',\n",
       " 'cf44',\n",
       " 'cf45',\n",
       " 'cf46',\n",
       " 'cf47',\n",
       " 'cf48',\n",
       " 'cg01',\n",
       " 'cg02',\n",
       " 'cg03',\n",
       " 'cg04',\n",
       " 'cg05',\n",
       " 'cg06',\n",
       " 'cg07',\n",
       " 'cg08',\n",
       " 'cg09',\n",
       " 'cg10',\n",
       " 'cg11',\n",
       " 'cg12',\n",
       " 'cg13',\n",
       " 'cg14',\n",
       " 'cg15',\n",
       " 'cg16',\n",
       " 'cg17',\n",
       " 'cg18',\n",
       " 'cg19',\n",
       " 'cg20',\n",
       " 'cg21',\n",
       " 'cg22',\n",
       " 'cg23',\n",
       " 'cg24',\n",
       " 'cg25',\n",
       " 'cg26',\n",
       " 'cg27',\n",
       " 'cg28',\n",
       " 'cg29',\n",
       " 'cg30',\n",
       " 'cg31',\n",
       " 'cg32',\n",
       " 'cg33',\n",
       " 'cg34',\n",
       " 'cg35',\n",
       " 'cg36',\n",
       " 'cg37',\n",
       " 'cg38',\n",
       " 'cg39',\n",
       " 'cg40',\n",
       " 'cg41',\n",
       " 'cg42',\n",
       " 'cg43',\n",
       " 'cg44',\n",
       " 'cg45',\n",
       " 'cg46',\n",
       " 'cg47',\n",
       " 'cg48',\n",
       " 'cg49',\n",
       " 'cg50',\n",
       " 'cg51',\n",
       " 'cg52',\n",
       " 'cg53',\n",
       " 'cg54',\n",
       " 'cg55',\n",
       " 'cg56',\n",
       " 'cg57',\n",
       " 'cg58',\n",
       " 'cg59',\n",
       " 'cg60',\n",
       " 'cg61',\n",
       " 'cg62',\n",
       " 'cg63',\n",
       " 'cg64',\n",
       " 'cg65',\n",
       " 'cg66',\n",
       " 'cg67',\n",
       " 'cg68',\n",
       " 'cg69',\n",
       " 'cg70',\n",
       " 'cg71',\n",
       " 'cg72',\n",
       " 'cg73',\n",
       " 'cg74',\n",
       " 'cg75',\n",
       " 'ch01',\n",
       " 'ch02',\n",
       " 'ch03',\n",
       " 'ch04',\n",
       " 'ch05',\n",
       " 'ch06',\n",
       " 'ch07',\n",
       " 'ch08',\n",
       " 'ch09',\n",
       " 'ch10',\n",
       " 'ch11',\n",
       " 'ch12',\n",
       " 'ch13',\n",
       " 'ch14',\n",
       " 'ch15',\n",
       " 'ch16',\n",
       " 'ch17',\n",
       " 'ch18',\n",
       " 'ch19',\n",
       " 'ch20',\n",
       " 'ch21',\n",
       " 'ch22',\n",
       " 'ch23',\n",
       " 'ch24',\n",
       " 'ch25',\n",
       " 'ch26',\n",
       " 'ch27',\n",
       " 'ch28',\n",
       " 'ch29',\n",
       " 'ch30',\n",
       " 'cj01',\n",
       " 'cj02',\n",
       " 'cj03',\n",
       " 'cj04',\n",
       " 'cj05',\n",
       " 'cj06',\n",
       " 'cj07',\n",
       " 'cj08',\n",
       " 'cj09',\n",
       " 'cj10',\n",
       " 'cj11',\n",
       " 'cj12',\n",
       " 'cj13',\n",
       " 'cj14',\n",
       " 'cj15',\n",
       " 'cj16',\n",
       " 'cj17',\n",
       " 'cj18',\n",
       " 'cj19',\n",
       " 'cj20',\n",
       " 'cj21',\n",
       " 'cj22',\n",
       " 'cj23',\n",
       " 'cj24',\n",
       " 'cj25',\n",
       " 'cj26',\n",
       " 'cj27',\n",
       " 'cj28',\n",
       " 'cj29',\n",
       " 'cj30',\n",
       " 'cj31',\n",
       " 'cj32',\n",
       " 'cj33',\n",
       " 'cj34',\n",
       " 'cj35',\n",
       " 'cj36',\n",
       " 'cj37',\n",
       " 'cj38',\n",
       " 'cj39',\n",
       " 'cj40',\n",
       " 'cj41',\n",
       " 'cj42',\n",
       " 'cj43',\n",
       " 'cj44',\n",
       " 'cj45',\n",
       " 'cj46',\n",
       " 'cj47',\n",
       " 'cj48',\n",
       " 'cj49',\n",
       " 'cj50',\n",
       " 'cj51',\n",
       " 'cj52',\n",
       " 'cj53',\n",
       " 'cj54',\n",
       " 'cj55',\n",
       " 'cj56',\n",
       " 'cj57',\n",
       " 'cj58',\n",
       " 'cj59',\n",
       " 'cj60',\n",
       " 'cj61',\n",
       " 'cj62',\n",
       " 'cj63',\n",
       " 'cj64',\n",
       " 'cj65',\n",
       " 'cj66',\n",
       " 'cj67',\n",
       " 'cj68',\n",
       " 'cj69',\n",
       " 'cj70',\n",
       " 'cj71',\n",
       " 'cj72',\n",
       " 'cj73',\n",
       " 'cj74',\n",
       " 'cj75',\n",
       " 'cj76',\n",
       " 'cj77',\n",
       " 'cj78',\n",
       " 'cj79',\n",
       " 'cj80',\n",
       " 'ck01',\n",
       " 'ck02',\n",
       " 'ck03',\n",
       " 'ck04',\n",
       " 'ck05',\n",
       " 'ck06',\n",
       " 'ck07',\n",
       " 'ck08',\n",
       " 'ck09',\n",
       " 'ck10',\n",
       " 'ck11',\n",
       " 'ck12',\n",
       " 'ck13',\n",
       " 'ck14',\n",
       " 'ck15',\n",
       " 'ck16',\n",
       " 'ck17',\n",
       " 'ck18',\n",
       " 'ck19',\n",
       " 'ck20',\n",
       " 'ck21',\n",
       " 'ck22',\n",
       " 'ck23',\n",
       " 'ck24',\n",
       " 'ck25',\n",
       " 'ck26',\n",
       " 'ck27',\n",
       " 'ck28',\n",
       " 'ck29',\n",
       " 'cl01',\n",
       " 'cl02',\n",
       " 'cl03',\n",
       " 'cl04',\n",
       " 'cl05',\n",
       " 'cl06',\n",
       " 'cl07',\n",
       " 'cl08',\n",
       " 'cl09',\n",
       " 'cl10',\n",
       " 'cl11',\n",
       " 'cl12',\n",
       " 'cl13',\n",
       " 'cl14',\n",
       " 'cl15',\n",
       " 'cl16',\n",
       " 'cl17',\n",
       " 'cl18',\n",
       " 'cl19',\n",
       " 'cl20',\n",
       " 'cl21',\n",
       " 'cl22',\n",
       " 'cl23',\n",
       " 'cl24',\n",
       " 'cm01',\n",
       " 'cm02',\n",
       " 'cm03',\n",
       " 'cm04',\n",
       " 'cm05',\n",
       " 'cm06',\n",
       " 'cn01',\n",
       " 'cn02',\n",
       " 'cn03',\n",
       " 'cn04',\n",
       " 'cn05',\n",
       " 'cn06',\n",
       " 'cn07',\n",
       " 'cn08',\n",
       " 'cn09',\n",
       " 'cn10',\n",
       " 'cn11',\n",
       " 'cn12',\n",
       " 'cn13',\n",
       " 'cn14',\n",
       " 'cn15',\n",
       " 'cn16',\n",
       " 'cn17',\n",
       " 'cn18',\n",
       " 'cn19',\n",
       " 'cn20',\n",
       " 'cn21',\n",
       " 'cn22',\n",
       " 'cn23',\n",
       " 'cn24',\n",
       " 'cn25',\n",
       " 'cn26',\n",
       " 'cn27',\n",
       " 'cn28',\n",
       " 'cn29',\n",
       " 'cp01',\n",
       " 'cp02',\n",
       " 'cp03',\n",
       " 'cp04',\n",
       " 'cp05',\n",
       " 'cp06',\n",
       " 'cp07',\n",
       " 'cp08',\n",
       " 'cp09',\n",
       " 'cp10',\n",
       " 'cp11',\n",
       " 'cp12',\n",
       " 'cp13',\n",
       " 'cp14',\n",
       " 'cp15',\n",
       " 'cp16',\n",
       " 'cp17',\n",
       " 'cp18',\n",
       " 'cp19',\n",
       " 'cp20',\n",
       " 'cp21',\n",
       " 'cp22',\n",
       " 'cp23',\n",
       " 'cp24',\n",
       " 'cp25',\n",
       " 'cp26',\n",
       " 'cp27',\n",
       " 'cp28',\n",
       " 'cp29',\n",
       " 'cr01',\n",
       " 'cr02',\n",
       " 'cr03',\n",
       " 'cr04',\n",
       " 'cr05',\n",
       " 'cr06',\n",
       " 'cr07',\n",
       " 'cr08',\n",
       " 'cr09']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6035ed10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c374304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cc0c74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5695c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d698c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b00177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of word types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227b1ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97a31f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4897750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.tagged_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ead46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the size of category “government”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e87d633d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70117"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words(categories='government'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12d33483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fdist = FreqDist(brown.words())\n",
    "fdist.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55126ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostfrequenttoken = FreqDist(brown.words())\n",
    "mostfrequenttoken.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "979bf074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostfrequenttoken.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d91883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdea36ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a00f05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2 Explore the corpora available in NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc1fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "fields = gutenberg.fileids()\n",
    "\n",
    "print(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e19e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Leaves of Grass by Walt Whitman 1855]\n",
      "\n",
      "\n",
      "Come, said my soul,\n",
      "Such verses for my Body let us write, (for we are one,)\n",
      "That should I after return,\n",
      "Or, long, long hence, in other spheres,\n",
      "There to some group of mates the chants resuming,\n",
      "(Tallying Earth's soil, trees, winds, tumultuous waves,)\n",
      "Ever with pleas'd smile I may keep on,\n",
      "Ever and ever yet the verses owning--as, first, I here and now\n",
      "Signing for Soul and Body, set to them my name,\n",
      "\n",
      "Walt Whitman\n",
      "\n",
      "\n",
      "\n",
      "[BOOK I.  INSCRIPTIONS]\n",
      "\n",
      "}  One's-Self I Sing\n",
      "\n",
      "One's-self I sing, a simple separate person,\n",
      "Yet utter the word Democratic, the word En-Masse.\n",
      "Of physiology from top to toe I sing,\n",
      "Not physiognomy alone nor brain alone is worthy for the Muse, I say\n",
      "    the Form complete is worthier far,\n",
      "The Female equally with the Male I sing.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "sample = gutenberg.raw(\"whitman-leaves.txt\")\n",
    "\n",
    "token = sent_tokenize(sample)\n",
    "\n",
    "for para in range(2):\n",
    "    print(token[para])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d20f0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154883"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.gutenberg.words('whitman-leaves.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddab4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43343e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c413766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"D:/nlp2_valorant.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab71a988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1deb8b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize \n\u001b[1;32m----> 2\u001b[0m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "sent_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051d43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
